{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорты "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open('../config.yaml', 'r') as f:\n",
    "    cfg = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from category_encoders import MEstimateEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "# from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Общая информация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(cfg['house_pricing']['train_dataset'])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(cfg['house_pricing']['test_dataset'])\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не все столбцы здесь выведены. Их список мы можем получить, используя аттрибут `columns`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почистим данные в нескольких столбцах, основываясь на data_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"Exterior2nd\"] = train_df[\"Exterior2nd\"].replace({\"Brk Cmn\": \"BrkComm\"})\n",
    "    # Some values of GarageYrBlt are corrupt, so we'll replace them\n",
    "    # with the year the house was built\n",
    "train_df[\"GarageYrBlt\"] = train_df[\"GarageYrBlt\"].where(train_df.GarageYrBlt <= 2010, train_df.YearBuilt)\n",
    "    # Names beginning with numbers are awkward to work with\n",
    "train_df.rename(columns={\n",
    "        \"1stFlrSF\": \"FirstFlrSF\",\n",
    "        \"2ndFlrSF\": \"SecondFlrSF\",\n",
    "        \"3SsnPorch\": \"Threeseasonporch\",\n",
    "        }, inplace=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df = train_df.select_dtypes(include=['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_df = train_df.select_dtypes(exclude=['object'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Коррелирующие признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(8, 5, figsize=(20, 20))\n",
    "axes_flattened = axes.reshape(-1)\n",
    "for i in range(len(num_df.columns)):\n",
    "    ax = axes_flattened[i]\n",
    "    sns.scatterplot(\n",
    "        x=num_df.iloc[:, i],\n",
    "        y='SalePrice',\n",
    "        data=num_df.dropna(),\n",
    "        ax=ax,\n",
    "    )\n",
    "fig.tight_layout(pad=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrplot(df, method=\"pearson\", annot=True, **kwargs):\n",
    "    sns.clustermap(\n",
    "        df.corr(method),\n",
    "        vmin=-1.0,\n",
    "        vmax=1.0,\n",
    "        cmap=\"icefire\",\n",
    "        method=\"complete\",\n",
    "        annot=annot,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "corrplot(num_df, annot=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из этой матрицы можно увидеть, какие столбцы сильно коррелируют между собой, например:\n",
    "1. GarageYrBlt и YearBuilt\n",
    "2. TotRmsAbvGrd и GrLivArea\n",
    "3. FirstFlrSF и TotalBsmtSF\n",
    "4. GarageArea и GarageCars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(\n",
    "    ['GarageYrBlt','TotRmsAbvGrd','FirstFlrSF','GarageCars'],\n",
    "    axis=1,\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Заполнение пустых значений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Может быть множество вариантов, при которых строка может содержать пустые значения. Например:\n",
    "1. Дом с 2 спальнями не может включать ответ на вопрос, насколько велика третья спальня\n",
    "2. Кто-то из опрошенных может не делиться своим доходом\n",
    "Библиотеки Python представляют недостающие числа как NaN-ми, что является сокращением от \"not a number\".\n",
    "\n",
    "Соберем статистику, связанную с NaN-ми. Какие ячейки имеют недостающие значения (в процентах), можно определить с помощью команды:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_df = (train_df.isnull().mean() * 100).reset_index()\n",
    "nan_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем этот датафрейм в более изящный вид:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_df.columns = [\"column_name\", \"percentage\"]\n",
    "nan_df.sort_values(\"percentage\", ascending=False, inplace=True)\n",
    "nan_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем квантили:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_srt = \"Number of columns with more than\"\n",
    "for percent in (80, 50, 20, 5):\n",
    "    print(f\"{intro_srt} {percent}% NANs: {(nan_df.percentage > percent).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем столбцы с более чем 80% NaN-в"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_columns = list(nan_df[nan_df.percentage > 80]['column_name'])\n",
    "nan_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Большинство моделей не умеют работать с NaN-ми. Поэтому требуется избавиться от них\n",
    "\n",
    "### Выброс стоблцов с NaN-ми"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 вариант - если, например, нужно выбросить одинаковые \n",
    "# столбцы для обучающей и тестовой выборок\n",
    "num_сols_with_missing = [col for col in num_df.columns \n",
    "                         if num_df[col].isnull().any()]\n",
    "num_сols_with_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(num_df.columns))\n",
    "num_df_dropped = num_df.drop(num_сols_with_missing, axis=1)\n",
    "print(len(num_df_dropped.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2 опция:** выбросить столбцы, напрямую используя `dropna()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(num_df.columns))\n",
    "num_df_dropped = num_df.dropna(axis=1)\n",
    "print(len(num_df_dropped.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если эти столбцы содержат полезную информацию (в местах, которые не были пропущены), модель теряет доступ к этой информации при удалении столбца. Кроме того, если тестовые данные имеют отсутствующие значения в тех местах, где тренировочные не имели, это приведет к ошибке.\n",
    "\n",
    "Так что обычно это не лучшее решение. Однако оно может быть полезно, когда большинство значений в столбце отсутствуют."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Заполнение недостающих значений каким-то значением\n",
    "\n",
    "Это значение будет не совсем правильным в большинстве случаев, но обычно оно дает более точные модели, чем полное удаление столбца."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Числовые признаки**\n",
    "\n",
    "Поведение по умолчанию заполняет столбец средним значением в заполненных ячейках. Существуют и более сложные стратегии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "my_imputer = SimpleImputer()\n",
    "\n",
    "filled_cols = my_imputer.fit_transform(train_df[num_сols_with_missing])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Альтернативно можно заполнить столбцы средним напрямую (или нулями, или чем угодно)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[num_сols_with_missing].fillna(train_df[num_сols_with_missing].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С точки зрения статистики такое заполнение оправдано, если все между признаками нет явной зависимости. В таком случае замена пропусков средними значениями не вносит смещения. Однако, часто условие независимости нарушается. В данном примере свойства домов сильно зависят от того, в каком районе они расположены. Поэтому средние значения лучше считать по районам.\n",
    "\n",
    "Взглянем на распределения средних значений по районам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh_grouped = train_df.groupby('Neighborhood')\n",
    "neigh_lot = (\n",
    "    neigh_grouped['LotFrontage'].mean()\n",
    "    .reset_index(name='LotFrontage_mean')\n",
    ")\n",
    "neigh_garage = (\n",
    "    neigh_grouped['GarageArea'].mean()\n",
    "    .reset_index(name='GarageArea_mean')\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1,2,figsize=(22,8))\n",
    "axes[0].tick_params(axis='x', rotation=90)\n",
    "sns.barplot(x='Neighborhood', y='LotFrontage_mean', data=neigh_lot, ax=axes[0])\n",
    "axes[1].tick_params(axis='x', rotation=90)\n",
    "sns.barplot(x='Neighborhood', y='GarageArea_mean', data=neigh_garage, ax=axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['LotFrontage'] = train_df.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.mean()))\n",
    "train_df['GarageArea'] = train_df.groupby('Neighborhood')['GarageArea'].transform(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заполним все оставшиеся числовые признаки средними (ранее мы не сохраняли результат в `train_df`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[num_сols_with_missing] = train_df[num_сols_with_missing].fillna(train_df[num_сols_with_missing].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Те столбцы, которые содержали более 80% NANов, удалим совсем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(nan_columns, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Категориальные (номинальные) признаки**\n",
    "\n",
    "Понятие среднего здесь тяжело использовать, поэтому проще заполнить модой, то есть наиболее часто встречающимся значением"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"MasVnrType\", \"MSZoning\", \"Exterior1st\", \"Exterior2nd\", \"SaleType\", \"Electrical\", \"Functional\"]\n",
    "for col in cols:\n",
    "    print(f\"Mode of column {col} is {train_df[col].dropna().mode()[0]}\")\n",
    "train_df[cols] = train_df.groupby(\"Neighborhood\")[cols].transform(lambda x: x.fillna(x.dropna().mode()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Порядковые признаки**\n",
    "\n",
    "Мы можем их заполнить средним или часто встречающимся, но также можно использовать значение по умолчанию \"NA\". Это значение будет удобно ассоциировать с нулем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = ['GarageType','GarageFinish','BsmtFinType2','BsmtExposure','BsmtFinType1', \n",
    "       'GarageCond','GarageQual','BsmtCond','BsmtQual','FireplaceQu',\"KitchenQual\",\n",
    "       \"HeatingQC\",'ExterQual','ExterCond']\n",
    "train_df[cat] = train_df[cat].fillna(\"NA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Удаление признаков со слабой вариативностью\n",
    "\n",
    "**Признаки с одним типичным значением**\n",
    "\n",
    "Некоторые признаки в основном состоят из одного значения или нулей, что не особо полезно для нас. Поэтому мы устанавливаем пороговое значение, определяемое пользователем, на уровне 96%. Если столбец имеет более 96% от одного и того же значения, мы считаем признак бесполезными и удалим его."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_almost_constant_columns(df, dropna=True):\n",
    "    cols = []\n",
    "    for i in df:\n",
    "        if dropna:\n",
    "            counts = df[i].dropna().value_counts()\n",
    "        else:\n",
    "            counts = df[i].value_counts()\n",
    "        most_popular_value_count = counts.iloc[0]\n",
    "        if (most_popular_value_count / len(df)) * 100 > 96:\n",
    "            cols.append(i)\n",
    "    return cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df = train_df.select_dtypes(include=['object'])\n",
    "overfit_cat = get_almost_constant_columns(cat_df)\n",
    "train_df = train_df.drop(overfit_cat, axis=1)\n",
    "overfit_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_df = train_df.select_dtypes(exclude=['object'])\n",
    "overfit_num = get_almost_constant_columns(num_df, dropna=True)\n",
    "train_df = train_df.drop(overfit_num, axis=1)\n",
    "overfit_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Признаки с маленькой дисперсией**\n",
    "\n",
    "Другой способ - использовать метод VarianceThreshold от sklearn — это простой базовый подход к выбору признаков. Он удаляет все признаки, дисперсия которых не соответствует определенному порогу. По умолчанию он удаляет все элементы с нулевой дисперсией, т.е. те элементы, которые имеют одинаковое значение у всех семплов.\n",
    "\n",
    "Стоит отметить, что дисперсия является абсолютной величиной, и выбор порога в этом случае является эмпирическим. При этом в общем случае малые значения дисперсии не говорят о бесполезности признака. Если признак задан на поле вещественных чисел, то его дискриминирующая способность не зависит от дисперсии, так как любой непрерывный интервал на вещественной оси содержит бесконечный набор значений. Однако, в случае дискретных значений (пример, целочисленных признаков) VarianceThreshold действительно становится полезным"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "sel = VarianceThreshold(threshold=0.1)\n",
    "num_col = train_df.select_dtypes(exclude=['object'])\n",
    "\n",
    "sel.fit(num_col)  # fit finds the features with low variance\n",
    "sum(sel.get_support())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод `get_support()` возвратит булевскую маску для признаков, которые проходят указанный порог по дисперсии. Ее можно использовать для отбора этих признаков "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel.get_support()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, таким образом мы получаем список всех признаков, которые были отсеяны данным алгоритмом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_col.columns[~sel.get_support()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Удаление выбросов\n",
    "\n",
    "Удаление выбросов предотвратит воздействие экстремальных значений на производительность наших моделей.\n",
    "\n",
    "Из скаттерплотов выше мы можем увидеть, что следующие признаки имеют экстремальные выбросы:\n",
    "\n",
    "* LotFrontage\n",
    "* LotArea\n",
    "* BsmtFinSF1\n",
    "* TotalBsmtSF\n",
    "* GrLivArea\n",
    "\n",
    "Мы уберем выбросы на основе определенного порогового значения.\n",
    "Эти значения мы получим из боксплотов (\"ящик с усиками\"):\n",
    "\n",
    "![Boxplot](boxplot.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_col = ['LotFrontage', 'LotArea', 'BsmtFinSF1', 'TotalBsmtSF', 'GrLivArea']\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 5))\n",
    "for ax, col in zip(axes, out_col):\n",
    "    sns.boxplot(y=train_df[col], data=train_df, ax=ax)\n",
    "fig.tight_layout(pad=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col, upper_bound in (\n",
    "    ('LotFrontage', 200),\n",
    "    ('LotArea', 100000),\n",
    "    ('BsmtFinSF1', 4000),\n",
    "    ('TotalBsmtSF', 5000),\n",
    "    ('GrLivArea', 4000),\n",
    "):\n",
    "    train_df = train_df.drop(train_df[train_df[col] > upper_bound].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После удаления выбросов, сильно коррелированных признаков и условных отсутствующих значений мы можем приступить к добавлению дополнительной информации для обучения нашей модели. Это делается с помощью - Feature Engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering - это техника, с помощью которой мы создаем новые признаки, которые потенциально могут помочь в прогнозировании нашей целевой переменной, которая в данном случае является SalePrice. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSSubClass - это столбец с числовым признаком, который на самом деле можно представить как категориальный"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['MSSubClass'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['MSSubClass'] = train_df['MSSubClass'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_map = {'Ex': 5,'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0}\n",
    "fintype_map = {'GLQ': 6,'ALQ': 5,'BLQ': 4,'Rec': 3,'LwQ': 2,'Unf': 1, 'NA': 0}\n",
    "expose_map = {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'NA': 0}\n",
    "fence_map = {'GdPrv': 4,'MnPrv': 3,'GdWo': 2, 'MnWw': 1,'NA': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_col = ['ExterQual','ExterCond','BsmtQual', 'BsmtCond','HeatingQC','KitchenQual','GarageQual','GarageCond', 'FireplaceQu']\n",
    "for col in ord_col:\n",
    "    train_df[col] = train_df[col].map(ordinal_map)\n",
    "    \n",
    "fin_col = ['BsmtFinType1','BsmtFinType2']\n",
    "for col in fin_col:\n",
    "    train_df[col] = train_df[col].map(fintype_map)\n",
    "\n",
    "train_df['BsmtExposure'] = train_df['BsmtExposure'].map(expose_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основываясь на текущих признаках, мы можем добавить первый дополнительный признак, который будет называться TotalLot и который суммирует LotFrontage и LotArea для определения общей площади земли, доступной в виде лота. Мы также можем рассчитать общее количество площади поверхности дома, TotalSF, сложив площадь от 1-го этажа и 2-го этажа. TotalBath также может быть использован, чтобы сказать нам в общей сложности, сколько ванных комнат есть в доме. Мы также можем добавить все различные типы крыльц вокруг дома и обобщить в общей площади крыльца, TotalPorch.\n",
    "\n",
    "* TotalLot = LotFrontage + LotArea\n",
    "* TotalSF = TotalBsmtSF + 2ndFlrSF\n",
    "* TotalBath = FullBath + HalfBath\n",
    "* TotalPorch = OpenPorchSF + EnclosedPorch + ScreenPorch\n",
    "* TotalBsmtFin = BsmtFinSF1 + BsmtFinSF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['TotalLot'] = train_df['LotFrontage'] + train_df['LotArea']\n",
    "train_df['TotalBsmtFin'] = train_df['BsmtFinSF1'] + train_df['BsmtFinSF2']\n",
    "train_df['TotalSF'] = train_df['TotalBsmtSF'] + train_df['SecondFlrSF']\n",
    "train_df['TotalBath'] = train_df['FullBath'] + train_df['HalfBath']\n",
    "train_df['TotalPorch'] = train_df['OpenPorchSF'] + train_df['EnclosedPorch'] + train_df['ScreenPorch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"LivLotRatio\"] = train_df[\"GrLivArea\"] / train_df[\"LotArea\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы также включаем создание бинарных столбцов для некоторых признаков, которые могут указывать на наличие(1) / отсутствие(0) некоторых признаков дома"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['MasVnrArea','TotalBsmtFin','TotalBsmtSF','SecondFlrSF','WoodDeckSF','TotalPorch']\n",
    "\n",
    "for col in cols:\n",
    "    col_name = col+'_bin'\n",
    "    train_df[col_name] = train_df[col].apply(lambda train_df: 1 if train_df > 0 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, поскольку нам нужны данные, которые носят численный характер, мы преобразуем оставшиеся категориальные столбцы с помощью one-hot-encoding с помощью метода get_dummies() в числовые столбцы, которые подходят для подачи в наш алгоритм машинного обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.get_dummies(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кластеринг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для создания новых признаков можно использовать обучение без учителя, например, кластеризацию с помощью k средних. Можно использовать как категорию (столбец с 0, 1, 2,...) метки кластеров или расстояние наблюдений до каждого кластера. Эти особенности иногда могут быть эффективными при распутывании сложных пространственных отношений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_features = [\n",
    "    \"LotArea\",\n",
    "    \"TotalBsmtSF\",\n",
    "    \"SecondFlrSF\",\n",
    "    \"GrLivArea\",\n",
    "]\n",
    "\n",
    "\n",
    "def cluster_labels(df, features, n_clusters=20):\n",
    "    X = df.copy()\n",
    "    X_scaled = X.loc[:, features]\n",
    "    X_scaled = (X_scaled - X_scaled.mean(axis=0)) / X_scaled.std(axis=0)\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=50, random_state=0)\n",
    "    X_new = pd.DataFrame()\n",
    "    X_new[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n",
    "    return X_new\n",
    "\n",
    "\n",
    "def cluster_distance(df, features, n_clusters=20):\n",
    "    X = df.copy()\n",
    "    X_scaled = X.loc[:, features]\n",
    "    X_scaled = (X_scaled - X_scaled.mean(axis=0)) / X_scaled.std(axis=0)\n",
    "    kmeans = KMeans(n_clusters=20, n_init=50, random_state=0)\n",
    "    X_cd = kmeans.fit_transform(X_scaled)\n",
    "    # Label features and join to dataset\n",
    "    X_cd = pd.DataFrame(\n",
    "        X_cd, columns=[f\"Centroid_{i}\" for i in range(X_cd.shape[1])]\n",
    "    )\n",
    "    return X_cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = cluster_labels(train_df, features=cluster_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['cluster_num'] = cluster_df[\"Cluster\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Скейлинг\n",
    "RobustScaler - это метод преобразования, который удаляет медиану и масштабирует данные в соответствии с диапазоном квантиля (по умолчанию IQR: межквартильный диапазон). IQR - это диапазон между 1-м квартилем (25-й квантилем) и 3 Квартиль (75-й квантиль). Он также устойчив к выпадающим значениям, что делает его идеальным для данных, где слишком много выпадающих значений, что резко сократит количество обучающих данных.\n",
    "\n",
    "Запуская скейлер как на тренировочном, так и на тестовом наборах, мы подвергаем себя проблеме утечки данных. Утечка данных - это проблема, когда для создания модели используется информация извне набора для обучения. Если мы подгоняем скейлер как на тренировочные, так и на тестовые данные, наши характеристики тренировочных данных будут содержать распределение нашего тестового набора. Таким образом, мы неявно передаем информацию о наших тестовых данных в окончательные тренировочные данные для обучения, что не даст нам возможности по-настоящему протестировать нашу модель на данных, которые она никогда не видела.\n",
    "\n",
    "*Извлеченные уроки:* Установка скалера только на обучающие данные, а затем преобразование данных как обучающей, так и тестовой выборок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "cols = train_df.select_dtypes(np.number).columns\n",
    "train_df = train_df.drop([\"Id\"], axis=1)\n",
    "transformer = RobustScaler().fit(train_df[cols])\n",
    "train_df[cols] = transformer.transform(train_df[cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, мы сильно преобразовали наш обучающий набор. кроме перчисленного, полезно использовать PCA, выбор признаков на основе информации и други методы. Как вы, наверное, заметили, все преобразования были сделаны только для тренировочного набора, но то же самое необходимо сделать и для тестового.\n",
    "\n",
    "Чтобы предотвратить утечку данных, все преобразования по среднему и тп нужно сделать независимо, а если мы, например, кодировали или удаляли столбцы, нужно сделать такое же преобразование, используя старые правила. \n",
    "\n",
    "После того, как это было сделано, можно передавать данные в модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2b3714695f2307aafe7da52bf6e53e38bc5469a267534973be7d21c816457eaf"
  },
  "kernelspec": {
   "display_name": "students",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
