{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практика по кластеризации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Настройка среды\n",
    "\n",
    "- Создаем изолированное окружение: `python -m venv venv`\n",
    "- Активируем: (unix) `. venv/bin/activate` или (win) `. venv/Scripts/activate`\n",
    "- Устанавливаем зависимости: `pip install -r practicum_8/requirements.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from functools import lru_cache\n",
    "from os.path import join as pjoin\n",
    "from typing import Any, Union\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "from ipywidgets import interact, fixed, IntSlider, FloatSlider\n",
    "from matplotlib import rcParams\n",
    "from numpy.typing import NDArray\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.cluster import DBSCAN, KMeans, MiniBatchKMeans\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "%matplotlib inline\n",
    "rcParams[\"font.size\"] = 14\n",
    "rcParams[\"figure.figsize\"] = 9, 8\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Используемые данные.\n",
    "Проточная цитометрия — метод исследования дисперсных сред в режиме поштучного анализа элементов дисперсной фазы по сигналам светорассеяния и флуоресценции. Название метода связано с основным приложением, а именно, с исследованием одиночных биологических клеток в потоке.\n",
    "<img src=\"misc/cytometry.png\" width=\"680\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../config.yaml\", \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "dfs: list[pd.DataFrame] = []\n",
    "data_dir = cfg[\"clustering\"][\"flow_cytometry\"]\n",
    "for file in os.listdir(data_dir):\n",
    "    df = pd.read_csv(pjoin(data_dir, file))\n",
    "    df = df.drop(columns=\"Time-\")\n",
    "    dfs.append(df)\n",
    "\n",
    "cols = dfs[0].columns\n",
    "clust_cols = [\"FSC-A-\", \"SSC-A-\"]  # 2 основных канала\n",
    "\n",
    "dfs[0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatterplot2d(\n",
    "    df: pd.DataFrame,\n",
    "    col1: str = \"FSC-A-\",\n",
    "    col2: str = \"SSC-A-\",\n",
    "    labels: Union[pd.Series, NDArray[np.int_]] = None,\n",
    "    dots_size: int = 5,\n",
    "    palette: str = \"coolwarm\",\n",
    ") -> None:\n",
    "    fig, _ = plt.subplots()\n",
    "    sns.scatterplot(x=df[col1], y=df[col2], hue=labels, s=dots_size, palette=palette)\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatterplot2d(dfs[0], cols[0], cols[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Немножко биологии:\n",
    "- 2 кластера внизу (видно же, что два!) -- это дебрис и лимфоциты соответственно\n",
    "- над ними центральный кластер -- моноциты\n",
    "- огромный кластер наверху -- базофилы + еще что-то (да какая разница?!)\n",
    "\n",
    "Попробуем тут и далее пытаться в первую очередь выделять лимфоциты. Именно разнообразные субпопуляции лимфоцитов чаще всего выделяют проточным цитометром, которые, в свою очередь, всесторонне характеризуют иммунную систему человека. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# избавимся от части выбросов в самом верху\n",
    "for idx, df in enumerate(dfs):\n",
    "    mask = (df[\"FSC-A-\"] > 200000) | (df[\"SSC-A-\"] > 240000)\n",
    "    dfs[idx] = df.drop(df[mask].index)\n",
    "\n",
    "scatterplot2d(dfs[0], cols[0], cols[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Основные алгоритмы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clust_and_viz(\n",
    "    df: pd.DataFrame,\n",
    "    clust_cols: list[str],\n",
    "    clusterer: Any,\n",
    "    dots_size: int = 5,\n",
    "    palette: str = \"coolwarm\",\n",
    "):\n",
    "    clusterer.fit(df[clust_cols])\n",
    "    labels = clusterer.labels_\n",
    "\n",
    "    print(f\"Число кластеров: {len(set(labels))}\")\n",
    "\n",
    "    scatterplot2d(\n",
    "        df=df,\n",
    "        col1=clust_cols[0],\n",
    "        col2=clust_cols[1],\n",
    "        labels=labels,\n",
    "        dots_size=dots_size,\n",
    "        palette=palette,\n",
    "    )\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(1.04, 1), loc=\"upper left\")\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нормализуем!\n",
    "df = dfs[0].copy()\n",
    "df_scaled = df.copy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_scaled[cols] = scaler.fit_transform(df_scaled[cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_kmeans = KMeans(n_clusters=4, n_init=10)\n",
    "base_kmeans_labels = clust_and_viz(\n",
    "    df=df_scaled, clust_cols=clust_cols, clusterer=base_kmeans\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_elbow_method(\n",
    "    df: pd.DataFrame,\n",
    "    clust_cols: list[str],\n",
    "    max_k: int = 25,\n",
    "    n_init: int = 10,\n",
    "    seed: int = SEED,\n",
    "    plot_results: bool = True,\n",
    ") -> int:\n",
    "    w_k = []\n",
    "    for k in range(1, max_k + 2):\n",
    "        kmeans = KMeans(n_clusters=k, n_init=n_init, random_state=seed).fit(\n",
    "            df[clust_cols]\n",
    "        )\n",
    "        w_k.append(np.sqrt(kmeans.inertia_))\n",
    "\n",
    "    # аналитический способ выбора оптимального количества кластеров\n",
    "    d_k = []\n",
    "    for idx in range(1, max_k):\n",
    "        d_k.append(abs(w_k[idx] - w_k[idx + 1]) / abs(w_k[idx - 1] - w_k[idx]))\n",
    "\n",
    "    if plot_results:\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        axs[0].plot(range(1, max_k + 2), w_k, marker=\"o\")\n",
    "        axs[0].set_xlabel(\"Число кластеров\")\n",
    "        axs[0].set_ylabel(\"W(K)\")\n",
    "        axs[1].plot(range(2, max_k + 1), d_k, marker=\"o\")\n",
    "        axs[1].set_xlabel(\"Число кластеров\")\n",
    "        axs[1].set_ylabel(\"D(K)\")\n",
    "\n",
    "    return np.argmax(d_k) + 2\n",
    "\n",
    "\n",
    "run_elbow_method(df=df_scaled, clust_cols=clust_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_kmeans = KMeans(n_clusters=24, n_init=25)\n",
    "opt_kmeans_labels = clust_and_viz(\n",
    "    df=df_scaled, clust_cols=clust_cols, clusterer=opt_kmeans\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Попробуем другой пик из графика метода локтя\n",
    "opt_kmeans = KMeans(n_clusters=6, n_init=25)\n",
    "opt_kmeans_labels = clust_and_viz(\n",
    "    df=df_scaled, clust_cols=clust_cols, clusterer=opt_kmeans\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dbscan = DBSCAN(eps=0.5, min_samples=5, metric=\"euclidean\", n_jobs=4)\n",
    "\n",
    "base_dbscan_labels = clust_and_viz(\n",
    "    df=df_scaled, clust_cols=clust_cols, clusterer=base_dbscan\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sorted_nn_dists(df: pd.DataFrame, min_pts: int = 4) -> None:\n",
    "    neighbors_fit = NearestNeighbors(n_neighbors=min_pts).fit(df)\n",
    "    distances, indices = neighbors_fit.kneighbors(df)\n",
    "    distances = np.sort(distances, axis=0)\n",
    "    distances = distances[:, 1]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(distances)\n",
    "\n",
    "\n",
    "plot_sorted_nn_dists(df_scaled[clust_cols], min_pts=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "opt_dbscan = DBSCAN(eps=0.04, min_samples=3, metric=\"euclidean\", n_jobs=4)\n",
    "\n",
    "opt_dbscan_labels = clust_and_viz(\n",
    "    # добавить palette=\"bright\" для лучшей видимости (но легенда становится дискретной)\n",
    "    df=df_scaled,\n",
    "    clust_cols=clust_cols,\n",
    "    clusterer=opt_dbscan,  # , palette=\"bright\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Counter(opt_dbscan_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# явно выделим лимфоциты\n",
    "scatterplot2d(df_scaled, clust_cols[0], clust_cols[1], labels=opt_dbscan_labels == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Интерактив!\n",
    "\n",
    "Создадим словарик с рассмотренными методами и ограничениями на их основные параметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = {\n",
    "    \"dbscan\": {\n",
    "        \"method\": DBSCAN,\n",
    "        \"params_range\": {\n",
    "            \"eps\": [*np.arange(0.1, 0.01, -0.01)],\n",
    "            \"min_samples\": [*range(25, 0, -1)],\n",
    "            \"metric\": [\"euclidean\", \"manhattan\"],\n",
    "            \"n_jobs\": [*range(1, 5), -1],\n",
    "        },\n",
    "    },\n",
    "    \"kmeans\": {\n",
    "        \"method\": KMeans,\n",
    "        \"params_range\": {\n",
    "            \"n_clusters\": [*range(2, 31)],\n",
    "            \"n_init\": [*range(3, 26)],\n",
    "            \"random_state\": fixed(SEED),\n",
    "        },\n",
    "    },\n",
    "    \"mbkmeans\": {\n",
    "        \"method\": MiniBatchKMeans,\n",
    "        \"params_range\": {\n",
    "            \"n_clusters\": [*range(2, 31)],\n",
    "            \"batch_size\": [*range(100, 1001, 100)],\n",
    "            \"n_init\": [*range(3, 26)],\n",
    "            \"random_state\": fixed(SEED),\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем класс, который будет кластеризовать данные и отрисовывать результаты в зависимости от поданных в его метод analysis2d параметров. Кэшируем результаты фит-предикта, чтобы не пересчитывать все заново, если, например, изменим размер точек на графике."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractiveClusterer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        method: Any,\n",
    "        params_range: dict[str, Any],\n",
    "        dfs: list[pd.DataFrame],\n",
    "        scaler: TransformerMixin = StandardScaler(),\n",
    "    ) -> None:\n",
    "        self.method = method\n",
    "        self.clusterer: Any = None\n",
    "        self.params_range = params_range\n",
    "        self.dfs = dfs\n",
    "        self.curr_df = None\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def fit_predict(\n",
    "        self,\n",
    "        patient: int = 0,\n",
    "        col1: str = \"FSC-A-\",\n",
    "        col2: str = \"SSC-A-\",\n",
    "        do_scaling: bool = False,\n",
    "        **kwargs,\n",
    "    ) -> NDArray[np.int_]:\n",
    "        self.clusterer = self.method(**kwargs)\n",
    "        self.clusterer.fit(self.curr_df)\n",
    "        return self.clusterer.labels_\n",
    "\n",
    "    def analysis2d(\n",
    "        self,\n",
    "        print_clust_num: bool = False,\n",
    "        dots_size: int = 5,\n",
    "        palette: str = \"coolwarm\",\n",
    "        patient: int = 0,\n",
    "        col1: str = \"FSC-A-\",\n",
    "        col2: str = \"SSC-A-\",\n",
    "        do_scaling: bool = True,\n",
    "        plot_scaled: bool = True,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        self.curr_df = self.dfs[patient][[col1, col2]].copy()\n",
    "\n",
    "        if do_scaling:\n",
    "            self.curr_df[self.curr_df.columns] = scaler.fit_transform(self.curr_df)\n",
    "\n",
    "        labels = self.fit_predict(\n",
    "            patient=patient, col1=col1, col2=col2, do_scaling=do_scaling, **kwargs\n",
    "        )\n",
    "\n",
    "        if print_clust_num:\n",
    "            print(\"Число кластеров:\", len(set(labels)))\n",
    "\n",
    "        scatterplot2d(\n",
    "            df=self.curr_df if plot_scaled else self.dfs[patient],\n",
    "            col1=col1,\n",
    "            col2=col2,\n",
    "            labels=labels,\n",
    "            dots_size=dots_size,\n",
    "            palette=palette,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Немного магии из коробки, юпитеровский виждет: https://ipywidgets.readthedocs.io/en/latest/examples/Using%20Interact.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "method_name = \"dbscan\"  # kmeans mbkmeans dbscan\n",
    "\n",
    "params_range = clustering[method_name][\"params_range\"]\n",
    "\n",
    "scaler = StandardScaler()  # MinMaxScaler()\n",
    "clusterer = InteractiveClusterer(**clustering[method_name], dfs=dfs, scaler=scaler)\n",
    "\n",
    "interact(\n",
    "    clusterer.analysis2d,\n",
    "    print_clust_num=True,\n",
    "    dots_size=[*range(1, 15)],\n",
    "    palette=[\"coolwarm\", \"bright\"],\n",
    "    patient=[*range(0, 5)],\n",
    "    col1=cols,\n",
    "    col2=cols,\n",
    "    do_scaling=[False, True],\n",
    "    plot_scaled=[False, True],\n",
    "    **params_range,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
