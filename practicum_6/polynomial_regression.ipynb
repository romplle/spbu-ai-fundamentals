{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Полиномиальная регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эта функция генерирует n узлов в интервале от -1 до 1, используя функцию f и шум (к значениям ординат мы прибавляет случайную величину со стандартным нормальным распределением)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(f, n=50):\n",
    "    x_nodes = np.linspace(-1., 1., 50)\n",
    "    sigma = 0.5\n",
    "    y_nodes = np.array([sigma * np.random.randn() + f(x) for x in x_nodes])\n",
    "    return x_nodes, y_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эта функция вычисляет сумму квадратов, которую мы минимизируем. Коротко мы называем ее RSS = residual sum of squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rss(X, y, a):\n",
    "    residual = y - X @ a\n",
    "    return np.dot(residual, residual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эта функция выводит на экран одновременно исходную функцию, сгенерированные узлы и кривую, полученную в результате полиномиальной регрессии. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regression(ax, f, x_nodes, y_nodes, fitting=None):\n",
    "    x_for_plotting = np.linspace(-1., 1., 200)\n",
    "    ax.plot(x_for_plotting, f(x_for_plotting), 'r', label=r'$\\tilde f(x)$', linewidth=2)\n",
    "    if fitting is not None:\n",
    "        ax.plot(x_for_plotting, fitting(x_for_plotting), 'g', label='$f(x)$', linewidth=2)\n",
    "    ax.plot(x_nodes, y_nodes, 'bo', label=r'$y_i$', markersize=8, alpha=0.3)\n",
    "    ax.set_xlabel(r'$x$', fontsize=16)\n",
    "    ax.set_ylabel(r'$y$', fontsize=16)\n",
    "    ax.legend(loc='lower right', fontsize=16)\n",
    "    ax.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сгенерируем обучающую выборку и выведем на экран ее вместе с исходной функцией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=42)\n",
    "f = lambda x: x**3 + x**2 + x\n",
    "x_nodes, y_nodes = generate_dataset(f, n=50)\n",
    "x_nodes = np.linspace(-1., 1., 50)\n",
    "x_for_plotting = np.linspace(-1., 1., 200)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13, 6))\n",
    "plot_regression(ax, f, x_nodes, y_nodes)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполним полиномиальную регрессию для степеней 1, 3, 7 и 15 и построим соответствующие графики. Обратите внимание на большие значения коэффициентов при больших n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**: Допишите функцию линейной регрессии, используя только numpy. Используйте нормальное уравнение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(13, 9))\n",
    "for ax, poly_degree in zip(axes.reshape(-1), (1, 3, 7, 15)):\n",
    "    X = np.vander(x_nodes, N=poly_degree+1, increasing=True)\n",
    "    a = np.linalg.inv(X.T @ X) @ X.T @ y_nodes \n",
    "    print(f'Степень полинома = {poly_degree}, RSS (сумма квадратов разностей) = {rss(X, y_nodes, a)}')\n",
    "    display(Math(r'f(x) = ' + '+'.join(['{:.4f}'.format(coeff) + monomial for monomial, coeff in zip(['', 'x'] + [r'x^{' + str(i) + '}' for i in range(2, poly_degree+1)], a)])))\n",
    "    print('\\n')\n",
    "    plot_regression(ax, f, x_nodes, y_nodes, \n",
    "                    fitting=lambda x: sum([a[i] * x**i for i in range(poly_degree + 1)]))\n",
    "    ax.set_title(f'Степень полинома: {poly_degree}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**: Проделйте то же самое, но используя LinearRegression из Sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(13, 9))\n",
    "for ax, poly_degree in zip(axes.reshape(-1), (1, 3, 7, 15)):\n",
    "    pipe = Pipeline(steps=[\n",
    "    ('preprocessor', PolynomialFeatures(degree=poly_degree, include_bias=False)),\n",
    "    ('estimator', LinearRegression())\n",
    "    ])\n",
    "    a = pipe.fit(x_nodes.reshape(-1,1), y_nodes).predict(x_nodes.reshape(-1,1))\n",
    "    print(f'Степень полинома = {poly_degree}, RSS (сумма квадратов разностей) = {rss(x_nodes, y_nodes, a)}')\n",
    "    display(Math(r'f(x) = ' + '+'.join(['{:.4f}'.format(coeff) + monomial for monomial, coeff in zip(['', 'x'] + [r'x^{' + str(i) + '}' for i in range(2, poly_degree+1)], a)])))\n",
    "    print('\\n')\n",
    "    plot_regression(ax, f, x_nodes, y_nodes, \n",
    "                    fitting=lambda x: pipe.predict(x.reshape(-1,1)))\n",
    "    ax.set_title(f'Степень полинома: {poly_degree}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся $L_2$-регуляризацией. Для примера возьмем $\\lambda = 0.1$. Используем буквально тем же кодом, но добавим теперь регуляризацию. Обратите внимание на то, что амплитуда коэффициентов резко снизилась"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**: Допишите функцию линейной регрессии c L2-регуляризацией, используя только numpy. Используйте нормальное уравнение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 0.1\n",
    "fig, axes = plt.subplots(2, 2, figsize=(13, 9))\n",
    "for ax, poly_degree in zip(axes.reshape(-1), (1, 3, 7, 15)):\n",
    "    X = np.vander(x_nodes, N=poly_degree+1, increasing=True)\n",
    "    a = np.linalg.inv(X.T @ X + lambda_*np.identity(poly_degree+1)) @ X.T @ y_nodes\n",
    "    print(f'Степень полинома = {poly_degree}')\n",
    "    display(Math(r'f(x) = ' + '+'.join(['{:.4f}'.format(coeff) + monomial for monomial, coeff in zip(['', 'x'] + [r'x^{' + str(i) + '}' for i in range(2, poly_degree+1)], a)])))\n",
    "    print('\\n')\n",
    "    plot_regression(ax, f, x_nodes, y_nodes, \n",
    "                    fitting=lambda x: sum([a[i] * x**i for i in range(poly_degree + 1)]))\n",
    "    ax.set_title(f'Степень полинома: {poly_degree}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы определить оптимальное значение гиперпараметра $\\lambda$, сгенерирует тестовую выборку (правильнее ее называть validation set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_nodes_for_validation, y_nodes_for_validation = generate_dataset(f, n=50)\n",
    "X_for_validation = np.vander(x_nodes_for_validation, N=poly_degree+1, increasing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для степени 15 выведем на экран зависимость оптимальной суммы квадратов от $\\lambda$. Минимизирующее значение $\\lambda$ является оптимальным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vander(x_nodes, N=poly_degree+1, increasing=True)\n",
    "poly_degree = 15\n",
    "lambda_values = np.logspace(-9, 1, 100)\n",
    "rss_values = np.zeros_like(lambda_values)\n",
    "for i, lambda_ in enumerate(lambda_values):\n",
    "    a = np.linalg.inv(X.T @ X + lambda_*np.identity(poly_degree+1)) @ X.T @ y_nodes\n",
    "    rss_values[i] = rss(X_for_validation, y_nodes_for_validation, a)\n",
    "fig, ax = plt.subplots(figsize=(13, 6))\n",
    "ax.semilogx(lambda_values, rss_values, linewidth=2)\n",
    "ax.set_xlabel(r'$\\lambda$', fontsize=16)\n",
    "ax.set_ylabel('RSS', fontsize=16)\n",
    "ax.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**: Допишите функцию линейной регрессии c L2-регуляризацией, но используя Sklearn. Используйте класс Ridge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(13, 9))\n",
    "for ax, poly_degree in zip(axes.reshape(-1), (1, 3, 7, 15)):\n",
    "    pipe = Pipeline(steps=[\n",
    "    ('preprocessor', PolynomialFeatures(degree=poly_degree, include_bias=False)),\n",
    "    ('estimator', Ridge())\n",
    "    ])\n",
    "    a = pipe.fit(x_nodes.reshape(-1,1), y_nodes).predict(x_nodes.reshape(-1,1))\n",
    "    print(f'Степень полинома = {poly_degree}, RSS (сумма квадратов разностей) = {rss(x_nodes, y_nodes, a)}')\n",
    "    display(Math(r'f(x) = ' + '+'.join(['{:.4f}'.format(coeff) + monomial for monomial, coeff in zip(['', 'x'] + [r'x^{' + str(i) + '}' for i in range(2, poly_degree+1)], a)])))\n",
    "    print('\\n')\n",
    "    plot_regression(ax, f, x_nodes, y_nodes, \n",
    "                    fitting=lambda x: pipe.predict(x.reshape(-1,1)))\n",
    "    ax.set_title(f'Степень полинома: {poly_degree}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Продемонстрируем теперь эффект от $L_1$-регуляризации. Ее особенность состоит в том, что она приводит к занулению как можно большего количества коэффициентов.\n",
    "\n",
    "Так как сама задача минимизации уже не решается аналитически, мы воспользуемся пакетом scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "poly_degree = 15\n",
    "X = np.vander(x_nodes, N=poly_degree+1, increasing=True)\n",
    "clf = linear_model.Lasso(alpha=0.03)\n",
    "clf.fit(X, y_nodes)\n",
    "a = clf.coef_\n",
    "print(f'Степень полинома = {poly_degree}')\n",
    "display(Math(r'f(x) = ' + '+'.join(['{:.4f}'.format(coeff) + monomial for monomial, coeff in zip(['', 'x'] + [r'x^{' + str(i) + '}' for i in range(2, poly_degree+1)], a)])))\n",
    "print('\\n')\n",
    "fig, ax = plt.subplots(figsize=(13, 6))\n",
    "plot_regression(ax, f, x_nodes, y_nodes, \n",
    "                fitting=lambda x: sum([a[i] * x**i for i in range(poly_degree + 1)]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
